{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from others.datasets import get_dataset\n",
    "from models.spectral_normalization_deflate_complex_both_bn import SpectralNorm\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('==========', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define regular train and test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, net, epoch, optimizer, scheduler, criterion, writer=None, model_path=\"./checkpoints/\"):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    global count_setp\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_idx = -1\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        count_setp += 1\n",
    "\n",
    "    writer.add_scalar('train/acc', 100.*correct/total, epoch)\n",
    "    writer.add_scalar('train/loss', train_loss/(batch_idx+1), epoch)\n",
    "    print('train/acc', 100.*correct/total)\n",
    "    print('train/loss', train_loss/(batch_idx+1))\n",
    "\n",
    "    scheduler.step()\n",
    "    return train_loss/(batch_idx+1), 100.*correct/total\n",
    "\n",
    "\n",
    "def test(testloader, net, epoch, criterion, optimizer, scheduler, writer=None, model_path=\"./checkpoints/\"):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_idx = -1\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    writer.add_scalar('test/acc', 100.*correct/total, epoch)\n",
    "    writer.add_scalar('test/loss', test_loss/(batch_idx+1), epoch)\n",
    "    print(' Test acc', 100.*correct/total)\n",
    "\n",
    "    return test_loss/(batch_idx+1), 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a simple convolutional model. To control the spectral norm of the convolutional and linear layer, we just wrap them in the SpectralNorm module which automatically keeps track of the largest singular value and bounds it to arbitrary values. To only keep track of the largest singular value without changing it, the clip_flag has to be set to False, and to store the spectral norm in at each iteration and make the plots in tensorboard, set the summary flag to True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, in_chan=1, out_chan=64, kernel_size=3, padding=1, width=28, writer=None):\n",
    "        super(ConvModel, self).__init__()\n",
    "        outdim = (width - kernel_size+2*padding) + 1\n",
    "        linear_input_size = outdim*outdim*out_chan\n",
    "\n",
    "        self.conv1 = SpectralNorm(nn.Conv2d(in_chan, out_chan, kernel_size=kernel_size, padding=padding), summary=True, writer=writer, clip_flag=True, clip=1.)\n",
    "        self.bn1 = SpectralNorm(nn.BatchNorm2d(out_chan), writer=writer, clip_flag=False, summary=True)\n",
    "        self.fc1 = SpectralNorm(nn.Linear(linear_input_size, 10), writer=writer, clip_flag=True, clip=1., summary=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model on MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "best_acc = 0  # best test accuracy\n",
    "count_setp = 0\n",
    "\n",
    "seed_val = 1\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "random.seed(seed_val)\n",
    "\n",
    "trainset = get_dataset('mnist', 'train')\n",
    "testset = get_dataset('mnist', 'test')\n",
    "trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, batch_size=128, num_workers=1)\n",
    "testloader = torch.utils.data.DataLoader(testset, shuffle=False, batch_size=128, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------> Output Directory:  simpleTest/\n",
      "Conv2d\n",
      "!!!!!!! Clipping is active !!!!!!!! clip val:  1.0\n",
      "BatchNorm2d\n",
      "Linear\n",
      "!!!!!!! Clipping is active !!!!!!!! clip val:  1.0\n",
      "\n",
      "Epoch: 0\n",
      "train/acc 87.025\n",
      "train/loss 0.4081151445410145\n",
      " Test acc 92.27\n",
      "\n",
      "Epoch: 1\n",
      "train/acc 92.24666666666667\n",
      "train/loss 0.246363132493074\n",
      " Test acc 93.01\n",
      "\n",
      "Epoch: 2\n",
      "train/acc 93.52333333333333\n",
      "train/loss 0.20507483705401675\n",
      " Test acc 93.26\n",
      "\n",
      "Epoch: 3\n",
      "train/acc 93.96666666666667\n",
      "train/loss 0.18773841408333544\n",
      " Test acc 93.47\n",
      "\n",
      "Epoch: 4\n",
      "train/acc 94.65833333333333\n",
      "train/loss 0.1681281467601816\n",
      " Test acc 94.66\n",
      "\n",
      "Epoch: 5\n",
      "train/acc 94.84833333333333\n",
      "train/loss 0.16057465913326247\n",
      " Test acc 94.91\n",
      "\n",
      "Epoch: 6\n",
      "train/acc 95.19\n",
      "train/loss 0.15159145486094297\n",
      " Test acc 94.74\n",
      "\n",
      "Epoch: 7\n",
      "train/acc 95.23833333333333\n",
      "train/loss 0.14695879793179822\n",
      " Test acc 94.51\n",
      "\n",
      "Epoch: 8\n",
      "train/acc 95.47666666666667\n",
      "train/loss 0.14061766603131537\n",
      " Test acc 94.83\n",
      "\n",
      "Epoch: 9\n",
      "train/acc 95.57166666666667\n",
      "train/loss 0.13725143281826332\n",
      " Test acc 92.43\n",
      "\n",
      "Epoch: 10\n",
      "train/acc 95.66333333333333\n",
      "train/loss 0.13428995663375615\n",
      " Test acc 94.99\n",
      "\n",
      "Epoch: 11\n",
      "train/acc 95.78666666666666\n",
      "train/loss 0.13024364574663422\n",
      " Test acc 94.42\n",
      "\n",
      "Epoch: 12\n",
      "train/acc 95.845\n",
      "train/loss 0.12854367319041732\n",
      " Test acc 95.32\n",
      "\n",
      "Epoch: 13\n",
      "train/acc 95.875\n",
      "train/loss 0.1288206982118552\n",
      " Test acc 94.46\n",
      "\n",
      "Epoch: 14\n",
      "train/acc 95.90166666666667\n",
      "train/loss 0.12528863889989314\n",
      " Test acc 95.24\n",
      "\n",
      "Epoch: 15\n",
      "train/acc 96.14833333333333\n",
      "train/loss 0.12170840263652649\n",
      " Test acc 95.26\n",
      "\n",
      "Epoch: 16\n",
      "train/acc 96.16666666666667\n",
      "train/loss 0.12095361726402219\n",
      " Test acc 94.7\n",
      "\n",
      "Epoch: 17\n",
      "train/acc 96.20166666666667\n",
      "train/loss 0.11863259320208894\n",
      " Test acc 94.47\n",
      "\n",
      "Epoch: 18\n",
      "train/acc 96.18166666666667\n",
      "train/loss 0.11830417400420602\n",
      " Test acc 94.55\n",
      "\n",
      "Epoch: 19\n",
      "train/acc 96.31166666666667\n",
      "train/loss 0.11632285196024345\n",
      " Test acc 94.76\n",
      "\n",
      "Epoch: 20\n",
      "train/acc 96.25666666666666\n",
      "train/loss 0.11591498985854802\n",
      " Test acc 94.87\n",
      "\n",
      "Epoch: 21\n",
      "train/acc 96.33\n",
      "train/loss 0.11325574981997898\n",
      " Test acc 95.4\n",
      "\n",
      "Epoch: 22\n",
      "train/acc 96.335\n",
      "train/loss 0.11351556433384606\n",
      " Test acc 95.51\n",
      "\n",
      "Epoch: 23\n",
      "train/acc 96.45666666666666\n",
      "train/loss 0.11259348744522533\n",
      " Test acc 95.33\n",
      "\n",
      "Epoch: 24\n",
      "train/acc 96.34666666666666\n",
      "train/loss 0.1110908736123332\n",
      " Test acc 95.57\n",
      "\n",
      "Epoch: 25\n",
      "train/acc 96.30333333333333\n",
      "train/loss 0.11349038006082526\n",
      " Test acc 94.19\n",
      "\n",
      "Epoch: 26\n",
      "train/acc 96.41666666666667\n",
      "train/loss 0.10982262000822818\n",
      " Test acc 94.88\n",
      "\n",
      "Epoch: 27\n",
      "train/acc 96.55333333333333\n",
      "train/loss 0.10743790760096203\n",
      " Test acc 94.74\n",
      "\n",
      "Epoch: 28\n",
      "train/acc 96.46833333333333\n",
      "train/loss 0.10972190271816783\n",
      " Test acc 94.69\n",
      "\n",
      "Epoch: 29\n",
      "train/acc 96.50166666666667\n",
      "train/loss 0.10803142444157143\n",
      " Test acc 95.43\n",
      "\n",
      "Epoch: 30\n",
      "train/acc 96.54666666666667\n",
      "train/loss 0.10784626880021238\n",
      " Test acc 95.25\n",
      "\n",
      "Epoch: 31\n",
      "train/acc 96.54333333333334\n",
      "train/loss 0.10660615143205311\n",
      " Test acc 95.41\n",
      "\n",
      "Epoch: 32\n",
      "train/acc 96.55\n",
      "train/loss 0.10688880722580561\n",
      " Test acc 95.38\n",
      "\n",
      "Epoch: 33\n",
      "train/acc 96.64666666666666\n",
      "train/loss 0.10493746961453068\n",
      " Test acc 94.24\n",
      "\n",
      "Epoch: 34\n",
      "train/acc 96.68666666666667\n",
      "train/loss 0.10548069775263384\n",
      " Test acc 94.73\n",
      "\n",
      "Epoch: 35\n",
      "train/acc 96.50666666666666\n",
      "train/loss 0.10726771494552398\n",
      " Test acc 94.33\n",
      "\n",
      "Epoch: 36\n",
      "train/acc 96.645\n",
      "train/loss 0.10417259051633287\n",
      " Test acc 95.66\n",
      "\n",
      "Epoch: 37\n",
      "train/acc 96.65\n",
      "train/loss 0.10395154563475774\n",
      " Test acc 95.45\n",
      "\n",
      "Epoch: 38\n",
      "train/acc 96.57\n",
      "train/loss 0.10564672946929932\n",
      " Test acc 95.63\n",
      "\n",
      "Epoch: 39\n",
      "train/acc 96.68666666666667\n",
      "train/loss 0.10285149050006734\n",
      " Test acc 95.6\n",
      "\n",
      "Epoch: 40\n",
      "train/acc 97.51666666666667\n",
      "train/loss 0.08429658809688681\n",
      " Test acc 95.71\n",
      "\n",
      "Epoch: 41\n",
      "train/acc 97.59666666666666\n",
      "train/loss 0.08545601167785588\n",
      " Test acc 95.81\n",
      "\n",
      "Epoch: 42\n",
      "train/acc 97.65833333333333\n",
      "train/loss 0.08379220657114154\n",
      " Test acc 95.79\n",
      "\n",
      "Epoch: 43\n",
      "train/acc 97.64666666666666\n",
      "train/loss 0.08291678324039937\n",
      " Test acc 95.81\n",
      "\n",
      "Epoch: 44\n",
      "train/acc 97.62166666666667\n",
      "train/loss 0.08211544154883066\n",
      " Test acc 95.94\n",
      "\n",
      "Epoch: 45\n",
      "train/acc 97.65666666666667\n",
      "train/loss 0.08144858255505816\n",
      " Test acc 95.74\n",
      "\n",
      "Epoch: 46\n",
      "train/acc 97.72333333333333\n",
      "train/loss 0.08088114311986132\n",
      " Test acc 95.78\n",
      "\n",
      "Epoch: 47\n",
      "train/acc 97.70166666666667\n",
      "train/loss 0.0811352807599535\n",
      " Test acc 95.85\n",
      "\n",
      "Epoch: 48\n",
      "train/acc 97.77833333333334\n",
      "train/loss 0.08020873382481049\n",
      " Test acc 95.77\n",
      "\n",
      "Epoch: 49\n",
      "train/acc 97.74666666666667\n",
      "train/loss 0.08018751525437273\n",
      " Test acc 95.86\n",
      "\n",
      "Epoch: 50\n",
      "train/acc 97.78166666666667\n",
      "train/loss 0.07995104398935843\n",
      " Test acc 95.88\n",
      "\n",
      "Epoch: 51\n",
      "train/acc 97.81333333333333\n",
      "train/loss 0.0792711949440589\n",
      " Test acc 95.72\n",
      "\n",
      "Epoch: 52\n",
      "train/acc 97.81666666666666\n",
      "train/loss 0.07900149010217139\n",
      " Test acc 95.69\n",
      "\n",
      "Epoch: 53\n",
      "train/acc 97.78\n",
      "train/loss 0.07925191437448266\n",
      " Test acc 95.8\n",
      "\n",
      "Epoch: 54\n",
      "train/acc 97.79\n",
      "train/loss 0.07922886457762865\n",
      " Test acc 95.84\n",
      "\n",
      "Epoch: 55\n",
      "train/acc 97.76333333333334\n",
      "train/loss 0.07945182795987836\n",
      " Test acc 95.67\n",
      "\n",
      "Epoch: 56\n",
      "train/acc 97.82833333333333\n",
      "train/loss 0.07882575548565718\n",
      " Test acc 95.91\n",
      "\n",
      "Epoch: 57\n",
      "train/acc 97.82166666666667\n",
      "train/loss 0.07823974342107265\n",
      " Test acc 95.98\n",
      "\n",
      "Epoch: 58\n",
      "train/acc 97.8\n",
      "train/loss 0.0780405007771401\n",
      " Test acc 95.81\n",
      "\n",
      "Epoch: 59\n",
      "train/acc 97.86333333333333\n",
      "train/loss 0.07797015996487029\n",
      " Test acc 95.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outdir = 'simpleTest/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "print('------------> Output Directory: ', outdir)\n",
    "writer = SummaryWriter(outdir) ## -> if you want to keep track of the largest singular values of the layers during the training, you need to pass this writer to the model. If not, you can just pass None.\n",
    "\n",
    "net = ConvModel(writer=writer) \n",
    "net = net.to(device)\n",
    "\n",
    "model_path =  outdir + 'ckpt.pth'\n",
    "model_path_test =  outdir + 'ckpt_best_test.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tr_loss, tr_acc = train(trainloader, net, epoch, optimizer, scheduler, criterion, writer=writer, model_path=model_path)\n",
    "    ts_loss, ts_acc = test(testloader, net, epoch, criterion, optimizer, scheduler, writer=writer, model_path=model_path_test)\n",
    "    net.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the singular values of each layer during training, use tensorboardX:\n",
    "\n",
    "```\n",
    "tensorboard --logdir simpleTest/ --port 6008\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition of conv layer and batch normalization: \n",
    "\n",
    "In the previosu model, we kept track of the spectral norm of the conv layer, dense layer, and batch normalization layer; however, we did not clip the spectral norm of the batch norm layer by setting the clip_flag to False. If this flag is set to True, the same approach as in Gouk et al. (2021) will be used. As pointed out in our paper, this method is not recommended as it impedes the training of the model and leads to very low accuracy on both training and test samples. Instead, we proposed the  application of our method to the composition of the convolutional layer and its succeeding batch norm layer. In order to do this for the previous model, we first make a helper module that represents the composition of the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNBN(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, padding=1, device='cpu', writer=None, bn=True):\n",
    "        super(CNNBN, self).__init__()\n",
    "        self.sub_conv1 = SpectralNorm(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, padding=padding), \n",
    "                                      device=device, clip_flag=True, clip=1., writer=writer, summary=True, identifier='_conv')\n",
    "        self.bn1 = SpectralNorm(nn.BatchNorm2d(out_planes, momentum=0.1, track_running_stats=True), \n",
    "                                device=device, clip_flag=False, clip=1., writer=writer, summary=True, identifier='_bn')\n",
    "        self.bn_flag = bn\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sub_conv1(x)\n",
    "        if self.bn_flag:\n",
    "            x = self.bn1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in CNNBN module, we wrapped the convolutional layer and the batch norm layer in the with the SpectralNorm module so that we can both keep trak of their spectral norms. By wrapping a CNNBN instance with our SpectralNorm module, we can keep track of the spectral norm of the composition of these two layers as well. For controlling the spectral norm of the composition of these layers, we follow the findings of our paper which recommends clipping the spectral norm of CNNBN and its constituent convolutional layer. This allows the batch normalization layer to have much larger spectral norm while controlling the spectral norm of the composition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel_v2(nn.Module):\n",
    "    def __init__(self, in_chan=1, out_chan=64, kernel_size=3, padding=1, width=28, writer=None, bn=True):\n",
    "        super(ConvModel_v2, self).__init__()\n",
    "        outdim = (width - kernel_size+2*padding) + 1\n",
    "        linear_input_size = outdim*outdim*out_chan\n",
    "\n",
    "        self.conv1_ = CNNBN(in_chan, out_chan, kernel_size=kernel_size, padding=padding, device=device, writer=writer, bn=bn)\n",
    "        self.conv1 = SpectralNorm(self.conv1_, device=device, clip_flag=True, clip=1., writer=writer, summary=True, identifier='_concat')\n",
    "\n",
    "        self.fc1 = SpectralNorm(nn.Linear(linear_input_size, 10), writer=writer, clip_flag=True, clip=1., summary=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same training procedure for the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------> Output Directory:  simpleTest_v2/\n",
      "!!!!!!! Clipping is active !!!!!!!! clip val:  1.0\n",
      "!!!!!!! Clipping is active !!!!!!!! clip val:  1.0\n",
      "Linear\n",
      "!!!!!!! Clipping is active !!!!!!!! clip val:  1.0\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/acc 87.025\n",
      "train/loss 0.4227368251474173\n",
      " Test acc 90.75\n",
      "\n",
      "Epoch: 1\n",
      "train/acc 86.475\n",
      "train/loss 0.6154305604475139\n",
      " Test acc 85.92\n",
      "\n",
      "Epoch: 2\n",
      "train/acc 82.58833333333334\n",
      "train/loss 0.6999289380716108\n",
      " Test acc 83.81\n",
      "\n",
      "Epoch: 3\n",
      "train/acc 82.99\n",
      "train/loss 0.6370966461167407\n",
      " Test acc 84.71\n",
      "\n",
      "Epoch: 4\n",
      "train/acc 83.45\n",
      "train/loss 0.6012959308080328\n",
      " Test acc 85.74\n",
      "\n",
      "Epoch: 5\n",
      "train/acc 83.415\n",
      "train/loss 0.6110366621632566\n",
      " Test acc 78.56\n",
      "\n",
      "Epoch: 6\n",
      "train/acc 83.38166666666666\n",
      "train/loss 0.6101108016108653\n",
      " Test acc 85.07\n",
      "\n",
      "Epoch: 7\n",
      "train/acc 83.42833333333333\n",
      "train/loss 0.609099855555146\n",
      " Test acc 80.09\n",
      "\n",
      "Epoch: 8\n",
      "train/acc 83.50166666666667\n",
      "train/loss 0.6029768122284651\n",
      " Test acc 84.29\n",
      "\n",
      "Epoch: 9\n",
      "train/acc 83.66166666666666\n",
      "train/loss 0.6020307180596821\n",
      " Test acc 86.01\n",
      "\n",
      "Epoch: 10\n",
      "train/acc 83.61666666666666\n",
      "train/loss 0.6053915920415158\n",
      " Test acc 82.03\n",
      "\n",
      "Epoch: 11\n",
      "train/acc 83.77666666666667\n",
      "train/loss 0.593336756168398\n",
      " Test acc 84.03\n",
      "\n",
      "Epoch: 12\n",
      "train/acc 83.92\n",
      "train/loss 0.5906891341148409\n",
      " Test acc 85.82\n",
      "\n",
      "Epoch: 13\n",
      "train/acc 83.84166666666667\n",
      "train/loss 0.5971684846669626\n",
      " Test acc 82.29\n",
      "\n",
      "Epoch: 14\n",
      "train/acc 83.87666666666667\n",
      "train/loss 0.5924877451935302\n",
      " Test acc 85.58\n",
      "\n",
      "Epoch: 15\n",
      "train/acc 83.94666666666667\n",
      "train/loss 0.58841030277423\n",
      " Test acc 81.99\n",
      "\n",
      "Epoch: 16\n",
      "train/acc 83.80166666666666\n",
      "train/loss 0.5964422133177328\n",
      " Test acc 85.0\n",
      "\n",
      "Epoch: 17\n",
      "train/acc 84.04166666666667\n",
      "train/loss 0.5896409566341433\n",
      " Test acc 85.49\n",
      "\n",
      "Epoch: 18\n",
      "train/acc 83.84\n",
      "train/loss 0.5970310487472681\n",
      " Test acc 80.83\n",
      "\n",
      "Epoch: 19\n",
      "train/acc 83.82333333333334\n",
      "train/loss 0.5964077095360135\n",
      " Test acc 85.57\n",
      "\n",
      "Epoch: 20\n",
      "train/acc 83.02833333333334\n",
      "train/loss 0.7882805567687509\n",
      " Test acc 84.43\n",
      "\n",
      "Epoch: 21\n",
      "train/acc 82.7\n",
      "train/loss 0.8012598492443435\n",
      " Test acc 83.14\n",
      "\n",
      "Epoch: 22\n",
      "train/acc 82.63333333333334\n",
      "train/loss 0.779940641256792\n",
      " Test acc 83.38\n",
      "\n",
      "Epoch: 23\n",
      "train/acc 82.58166666666666\n",
      "train/loss 0.7754438687235053\n",
      " Test acc 79.76\n",
      "\n",
      "Epoch: 24\n",
      "train/acc 82.39333333333333\n",
      "train/loss 0.7722649782705409\n",
      " Test acc 82.82\n",
      "\n",
      "Epoch: 25\n",
      "train/acc 82.3\n",
      "train/loss 0.7664996325842607\n",
      " Test acc 83.41\n",
      "\n",
      "Epoch: 26\n",
      "train/acc 82.29666666666667\n",
      "train/loss 0.7628088969665804\n",
      " Test acc 80.79\n",
      "\n",
      "Epoch: 27\n",
      "train/acc 82.24\n",
      "train/loss 0.7629224520756491\n",
      " Test acc 82.55\n",
      "\n",
      "Epoch: 28\n",
      "train/acc 82.18666666666667\n",
      "train/loss 0.7624217464979778\n",
      " Test acc 83.82\n",
      "\n",
      "Epoch: 29\n",
      "train/acc 82.15333333333334\n",
      "train/loss 0.7644527177058303\n",
      " Test acc 81.87\n",
      "\n",
      "Epoch: 30\n",
      "train/acc 82.025\n",
      "train/loss 0.7591761500596492\n",
      " Test acc 82.39\n",
      "\n",
      "Epoch: 31\n",
      "train/acc 82.03833333333333\n",
      "train/loss 0.7630878417476662\n",
      " Test acc 80.22\n",
      "\n",
      "Epoch: 32\n",
      "train/acc 82.035\n",
      "train/loss 0.7574402731873079\n",
      " Test acc 82.16\n",
      "\n",
      "Epoch: 33\n",
      "train/acc 81.94333333333333\n",
      "train/loss 0.7585563414386595\n",
      " Test acc 82.97\n",
      "\n",
      "Epoch: 34\n",
      "train/acc 81.89166666666667\n",
      "train/loss 0.759714670526956\n",
      " Test acc 80.95\n",
      "\n",
      "Epoch: 35\n",
      "train/acc 81.85166666666667\n",
      "train/loss 0.7569137327452459\n",
      " Test acc 82.2\n",
      "\n",
      "Epoch: 36\n",
      "train/acc 81.80166666666666\n",
      "train/loss 0.7575806402194221\n",
      " Test acc 79.77\n",
      "\n",
      "Epoch: 37\n",
      "train/acc 81.83166666666666\n",
      "train/loss 0.7560274054500848\n",
      " Test acc 81.53\n",
      "\n",
      "Epoch: 38\n",
      "train/acc 81.69333333333333\n",
      "train/loss 0.7567938803863932\n",
      " Test acc 82.37\n",
      "\n",
      "Epoch: 39\n",
      "train/acc 81.78\n",
      "train/loss 0.7566297926119904\n",
      " Test acc 79.2\n",
      "\n",
      "Epoch: 40\n",
      "train/acc 81.12833333333333\n",
      "train/loss 0.8870889272516979\n",
      " Test acc 82.01\n",
      "\n",
      "Epoch: 41\n",
      "train/acc 81.04666666666667\n",
      "train/loss 0.9064618933683773\n",
      " Test acc 82.05\n",
      "\n",
      "Epoch: 42\n",
      "train/acc 80.99833333333333\n",
      "train/loss 0.9064786927278108\n",
      " Test acc 81.62\n",
      "\n",
      "Epoch: 43\n",
      "train/acc 81.075\n",
      "train/loss 0.8981524671572866\n",
      " Test acc 81.88\n",
      "\n",
      "Epoch: 44\n",
      "train/acc 81.115\n",
      "train/loss 0.9051552170883618\n",
      " Test acc 81.01\n",
      "\n",
      "Epoch: 45\n",
      "train/acc 81.115\n",
      "train/loss 0.9036439017954666\n",
      " Test acc 81.87\n",
      "\n",
      "Epoch: 46\n",
      "train/acc 81.06666666666666\n",
      "train/loss 0.8967135762100789\n",
      " Test acc 82.0\n",
      "\n",
      "Epoch: 47\n",
      "train/acc 81.10166666666667\n",
      "train/loss 0.9001394367929715\n",
      " Test acc 81.57\n",
      "\n",
      "Epoch: 48\n",
      "train/acc 81.06166666666667\n",
      "train/loss 0.8979355276012218\n",
      " Test acc 81.88\n",
      "\n",
      "Epoch: 49\n",
      "train/acc 81.05333333333333\n",
      "train/loss 0.8979014723794039\n",
      " Test acc 80.56\n",
      "\n",
      "Epoch: 50\n",
      "train/acc 81.03833333333333\n",
      "train/loss 0.9009178461296472\n",
      " Test acc 81.67\n",
      "\n",
      "Epoch: 51\n",
      "train/acc 81.015\n",
      "train/loss 0.8974829868975479\n",
      " Test acc 81.88\n",
      "\n",
      "Epoch: 52\n",
      "train/acc 81.0\n",
      "train/loss 0.8971359804749235\n",
      " Test acc 81.57\n",
      "\n",
      "Epoch: 53\n",
      "train/acc 81.04666666666667\n",
      "train/loss 0.8961520515271088\n",
      " Test acc 81.74\n",
      "\n",
      "Epoch: 54\n",
      "train/acc 81.00166666666667\n",
      "train/loss 0.8983221451865077\n",
      " Test acc 81.86\n",
      "\n",
      "Epoch: 55\n",
      "train/acc 81.09333333333333\n",
      "train/loss 0.9038214921188761\n",
      " Test acc 81.46\n",
      "\n",
      "Epoch: 56\n",
      "train/acc 81.05\n",
      "train/loss 0.8979090102700028\n",
      " Test acc 81.84\n",
      "\n",
      "Epoch: 57\n",
      "train/acc 81.0\n",
      "train/loss 0.8955975326139536\n",
      " Test acc 81.63\n",
      "\n",
      "Epoch: 58\n",
      "train/acc 81.03\n",
      "train/loss 0.9011322735215047\n",
      " Test acc 81.56\n",
      "\n",
      "Epoch: 59\n",
      "train/acc 81.01333333333334\n",
      "train/loss 0.8911014862660406\n",
      " Test acc 81.7\n",
      "\n",
      "Epoch: 60\n",
      "train/acc 80.55666666666667\n",
      "train/loss 0.9901173412164391\n",
      " Test acc 81.97\n",
      "\n",
      "Epoch: 61\n",
      "train/acc 80.48333333333333\n",
      "train/loss 0.9901351790184151\n",
      " Test acc 82.18\n",
      "\n",
      "Epoch: 62\n",
      "train/acc 80.48166666666667\n",
      "train/loss 0.9868806211678967\n",
      " Test acc 82.01\n",
      "\n",
      "Epoch: 63\n",
      "train/acc 80.44\n",
      "train/loss 0.9920773445161929\n",
      " Test acc 81.9\n",
      "\n",
      "Epoch: 64\n",
      "train/acc 80.44333333333333\n",
      "train/loss 0.9904284262453823\n",
      " Test acc 82.09\n",
      "\n",
      "Epoch: 65\n",
      "train/acc 80.44833333333334\n",
      "train/loss 0.9863509226963718\n",
      " Test acc 81.05\n",
      "\n",
      "Epoch: 66\n",
      "train/acc 80.42166666666667\n",
      "train/loss 0.9923027306477398\n",
      " Test acc 81.74\n",
      "\n",
      "Epoch: 67\n",
      "train/acc 80.415\n",
      "train/loss 0.9936343345052397\n",
      " Test acc 81.82\n",
      "\n",
      "Epoch: 68\n",
      "train/acc 80.47166666666666\n",
      "train/loss 0.9889739033764106\n",
      " Test acc 81.92\n",
      "\n",
      "Epoch: 69\n",
      "train/acc 80.41333333333333\n",
      "train/loss 0.990765367997989\n",
      " Test acc 81.78\n",
      "\n",
      "Epoch: 70\n",
      "train/acc 80.4\n",
      "train/loss 0.9917920624523529\n",
      " Test acc 81.75\n",
      "\n",
      "Epoch: 71\n",
      "train/acc 80.47333333333333\n",
      "train/loss 0.9898472866778181\n",
      " Test acc 81.87\n",
      "\n",
      "Epoch: 72\n",
      "train/acc 80.41\n",
      "train/loss 0.9888215913955591\n",
      " Test acc 81.72\n",
      "\n",
      "Epoch: 73\n",
      "train/acc 80.45166666666667\n",
      "train/loss 0.9914538241398614\n",
      " Test acc 81.76\n",
      "\n",
      "Epoch: 74\n",
      "train/acc 80.415\n",
      "train/loss 0.9881542092447342\n",
      " Test acc 81.83\n",
      "\n",
      "Epoch: 75\n",
      "train/acc 80.42666666666666\n",
      "train/loss 0.9854703372729613\n",
      " Test acc 81.64\n",
      "\n",
      "Epoch: 76\n",
      "train/acc 80.40333333333334\n",
      "train/loss 0.9913110177654193\n",
      " Test acc 81.71\n",
      "\n",
      "Epoch: 77\n",
      "train/acc 80.42\n",
      "train/loss 0.9871804249057892\n",
      " Test acc 81.89\n",
      "\n",
      "Epoch: 78\n",
      "train/acc 80.42333333333333\n",
      "train/loss 0.984165234733492\n",
      " Test acc 79.86\n",
      "\n",
      "Epoch: 79\n",
      "train/acc 80.37833333333333\n",
      "train/loss 0.9914832772222409\n",
      " Test acc 81.74\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 80\n",
    "outdir = 'simpleTest_v2/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "print('------------> Output Directory: ', outdir)\n",
    "writer = SummaryWriter(outdir) ## -> if you want to keep track of the largest singular values of the layers during the training, you need to pass this writer to the model. If not, you can just pass None.\n",
    "\n",
    "net = ConvModel_v2(writer=writer) \n",
    "net = net.to(device)\n",
    "\n",
    "model_path =  outdir + 'ckpt.pth'\n",
    "model_path_test =  outdir + 'ckpt_best_test.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tr_loss, tr_acc = train(trainloader, net, epoch, optimizer, scheduler, criterion, writer=writer, model_path=model_path)\n",
    "    ts_loss, ts_acc = test(testloader, net, epoch, criterion, optimizer, scheduler, writer=writer, model_path=model_path_test)\n",
    "    net.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To visualize the singular values of each layer during training, use tensorboardX:\n",
    "\n",
    "```\n",
    "tensorboard --logdir simpleTest_v2/ --port 6008\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of our method to the concatenation of convolutional and batch norm layer might not be as stable as the application of our method to individual layers and might need additional effort observing the model's behavior and tuning the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the detail of our method, comprehensive results and discussions, please refer to our paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Miyato, Takeru, et al. \"Spectral normalization for generative adversarial networks.\" arXiv preprint arXiv:1802.05957 (2018).\n",
    "2. Gouk, Henry, et al. \"Regularisation of neural networks by enforcing lipschitz continuity.\" Machine Learning 110 (2021): 393-416.\n",
    "3. Senderovich, Alexandra, et al. \"Towards practical control of singular values of convolutional layers.\" Advances in Neural Information Processing Systems 35 (2022): 10918-10930.\n",
    "4. Delattre, Blaise, et al. \"Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration.\" arXiv preprint arXiv:2305.16173 (2023).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the models (ResNet18 and DLA) and training them on cifar-10 we used codes from this repository: https://github.com/kuangliu/pytorch-cifar/tree/master\n",
    "\n",
    "For the adversarial attacks and and MNIST data, we used the code from this repository: https://github.com/AI-secure/Transferability-Reduced-Smooth-Ensemble/tree/main "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
